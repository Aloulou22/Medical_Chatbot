# Medical-Chatbot

## Overview

Medical-Chatbot is a Streamlit-based web application that allows users to interact with a medical chatbot powered by an advanced language model (LLM) and a vector knowledge base. The goal is to provide accurate responses based on indexed medical documentation.

## Key Features

- **Interactive User Interface**: Utilizes Streamlit for a user-friendly chat experience.
- **Contextual Search**: User queries are enriched through semantic search in a vector database (FAISS).
- **Response Generation**: Responses are generated by an LLM (Mistral-7B-Instruct-v0.3 via HuggingFace) based solely on the extracted context.
- **History Management**: Maintains a history of exchanges in the user session.
- **Source Display**: Displays the source documents used to generate the response for transparency.

## Project Architecture

```
medibot.py           # Main Streamlit script
vectorstore/db_faiss # FAISS vector database (embeddings)
```

### Workflow

1. **Vector Database Loading**:
   - Uses FAISS to store and retrieve embeddings of medical documents.
   - Embeddings are generated using the "all-MiniLM-L6-v2" model.
2. **User Input**:
   - Users pose questions through the Streamlit interface.
3. **Semantic Search**:
   - The most relevant documents are extracted from the vector database.
4. **Response Generation**:
   - The LLM (Mistral-7B-Instruct-v0.3) generates a response based solely on the extracted context.
   - A custom prompt guides the model to avoid out-of-context responses.
5. **Display**:
   - The response and sources are displayed in the interface.

## Main Dependencies

- `streamlit`
- `langchain`
- `langchain_huggingface`
- `langchain_community`
- `faiss-cpu`
- `sentence-transformers`

## Launching the Project

1. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```
2. Launch the application:
   ```bash
   streamlit run medibot.py
   ```

## Security and Privacy
- The chatbot only provides responses based on the indexed document context.
- If a response is not found in the database, the bot explicitly indicates this.

## Notes
- The HuggingFace token must be valid to access the remote model.
- The FAISS vector database must be pre-built and placed in `vectorstore/db_faiss`. 
